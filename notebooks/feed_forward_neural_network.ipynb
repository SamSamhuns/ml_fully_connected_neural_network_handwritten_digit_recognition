{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "We will use the lower resolution MINST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Sequence\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits              # The MNIST data set\n",
    "from sklearn.preprocessing import StandardScaler      # It is important in neural networks to scale the date\n",
    "from sklearn.model_selection import train_test_split  # The standard train test split to prevent overfitting and choose hyperparameters\n",
    "\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the digits dataset: (1797, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load all the digits dataset from the sklearn library\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "print(\"The shape of the digits dataset:\", X.shape)\n",
    "\n",
    "plt.gray()  # set colormap to gray\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the dataset\n",
    "To speed the weight convergence, the training features must be scaled to have a mean of 0 and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.33501649, -0.04308102, ..., -1.14664746,\n",
       "        -0.5056698 , -0.19600752],\n",
       "       [ 0.        , -0.33501649, -1.09493684, ...,  0.54856067,\n",
       "        -0.5056698 , -0.19600752],\n",
       "       [ 0.        , -0.33501649, -1.09493684, ...,  1.56568555,\n",
       "         1.6951369 , -0.19600752],\n",
       "       ...,\n",
       "       [ 0.        , -0.33501649, -0.88456568, ..., -0.12952258,\n",
       "        -0.5056698 , -0.19600752],\n",
       "       [ 0.        , -0.33501649, -0.67419451, ...,  0.8876023 ,\n",
       "        -0.5056698 , -0.19600752],\n",
       "       [ 0.        , -0.33501649,  1.00877481, ...,  0.8876023 ,\n",
       "        -0.26113572, -0.19600752]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the StandardScaler to standardize our X_feat\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Looking the new features after scaling\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and test datasets\n",
    "\n",
    "We split the data into training and test data sets with 60-40 split. \n",
    "\n",
    "We will train the neural network with the training dataset, and evaluate our neural network with the test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.40, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_to_vect(y):\n",
    "    \"\"\"\n",
    "    One Hot Encoding for target\n",
    "    Converts into a 10 element array\n",
    "    \n",
    "    y can be an integer or a Sequence of integers\n",
    "    \"\"\"\n",
    "    if isinstance(y, int):\n",
    "        return np.array([1 if i==y else 0 for i in range(10)])\n",
    "    elif isinstance(y, Sequence) or isinstance(y, np.ndarray):\n",
    "        return np.array([[1 if i==target else 0 for i in range(10)] for target in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the training and test targets to vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert digits to vectors using the func above\n",
    "y_train_vect, y_test_vect = convert_y_to_vect(y_train), convert_y_to_vect(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5 4 4]\n",
      "[[0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    print(y_train[0:4])\n",
    "    print((y_train_vect[0:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The activation function and its derivative\n",
    "\n",
    "We use the Sigmoid Activation function\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "And use its derivative\n",
    "\n",
    "$\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.e**(-z))\n",
    "\n",
    "def sigmoid_deriv(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and initializing W and b\n",
    "\n",
    "W = weights of the neurons\n",
    "\n",
    "b = biases of the neurons\n",
    "\n",
    "The weights in W are different so that during back propagation, the nodes on a level will have different gradients and thus have different update values.\n",
    "\n",
    "The weights are randomly initialized from the uniform range \\[0.0, 1.0\\). The weights have to be small as the sigmoid flats out for large inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_and_bias(nn_structure: List[int]):\n",
    "    \"\"\"\n",
    "    nn_structure is a list that reprs the number of neurons in the NN layer\n",
    "    We use the Kaiming Initialization for the weights\n",
    "    \"\"\"\n",
    "    weights, bias = {}, {}\n",
    "    \n",
    "    # first layer is input layer so we do not save weights for it\n",
    "    for layer, n_neurons in enumerate(nn_structure[1:], start=1):\n",
    "        weights[layer] = np.random.random((n_neurons, nn_structure[layer-1])) / np.sqrt(n_neurons)\n",
    "        bias[layer] = np.random.random(n_neurons)\n",
    "        \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the weight and bias gradient dicts $\\triangledown W$ and $\\triangledown b$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_and_bias_gradients(nn_structure: List[int]):\n",
    "    delta_W, delta_b = {}, {}\n",
    "    \n",
    "    for layer, n_neurons in enumerate(nn_structure[1:], start=1):\n",
    "        delta_W[layer] = np.zeros((n_neurons, nn_structure[layer-1]))\n",
    "        delta_b[layer] = np.zeros(n_neurons)\n",
    "        \n",
    "    return  delta_W, delta_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward\n",
    "\n",
    "The feed_forward function returns the values of $a$ and $z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, W, b):\n",
    "    \"\"\"\n",
    "    x is a single data point / 1D array\n",
    "    a,z ignore input layer values\n",
    "    \"\"\"\n",
    "    # a in layer 1 is the input x itself\n",
    "    a = {1:x} # holds values of 'a' for all layers, a = func_activation(z)\n",
    "    z = {}    # holds values of 'z' for all layers, z = Wx + b\n",
    "    X_feat = x\n",
    "\n",
    "    for layer, (weight,bias) in enumerate(zip(W, b), start=2): \n",
    "        z[layer] = W[layer-1].dot(X_feat) + b[layer-1]\n",
    "        a[layer] = sigmoid(z[layer])\n",
    "        X_feat = a[layer]\n",
    "        \n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing $\\delta$\n",
    "\n",
    "$\\delta^{(s_l)}$ is computed in \"calculate_out_layer_delta\"  \n",
    "\n",
    "$delta^{nl} = -(y_i - a_i^{nl}) * f'(z_i^{nl})$\n",
    "\n",
    "$\\delta^{(\\ell)}$ is computed for the hidden layers in \"calculate_hidden_delta\" \n",
    "    \n",
    "$delta^{l} = (transpose(W^{l}) * delta^{l+1}) * f'(z^{l})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    # delta^(nl) = -(y_i - a_i^(nl)) * f'(z_i^(nl))\n",
    "    return  -(y-a_out) * sigmoid_deriv(z_out)\n",
    "\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return (w_l.T@delta_plus_1) * sigmoid_deriv(z_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Back Propagation Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25):\n",
    "    \"\"\"\n",
    "    X msut be np.ndarray\n",
    "    \"\"\"\n",
    "    W, b = init_weight_and_bias(nn_structure)\n",
    "    epoch = 0\n",
    "    m, n = X.shape\n",
    "    mse_cost_overtime = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    # while the counter is less than the max iterations:\n",
    "    for epoch in tqdm(range(iter_num)):\n",
    "        delta_W, delta_b = init_weight_and_bias_gradients(nn_structure)\n",
    "        cost = 0\n",
    "        \n",
    "        for data_idx, data in enumerate(X):\n",
    "            # feed forward pass saves a and z values to be used in gradient descent\n",
    "            a, z = feed_forward(data, W, b)\n",
    "            deltas = {}\n",
    "            # loop from n-1 to 1 backpropagating the errors\n",
    "            for layer in range(len(nn_structure),0,-1):\n",
    "                if layer == len(nn_structure): # layer is output layer\n",
    "                    deltas[layer] = calculate_out_layer_delta(y[data_idx,:], a[layer], z[layer])\n",
    "                    # squared error calculation\n",
    "                    cost += np.sqrt(np.sum((a[layer] - y[data_idx,:])**2))\n",
    "                else:\n",
    "                    if layer > 1: # layer is hidden layer\n",
    "                        deltas[layer] = calculate_hidden_delta(deltas[layer+1], W[layer], z[layer])\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(a^(l))\n",
    "                    delta_W[layer] +=  (deltas[layer+1][:, np.newaxis] @ a[layer][:,np.newaxis].T)\n",
    "                    \n",
    "                    # hint: you can use np.newaxis to increase the number of dimensions\n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    delta_b[layer] +=  deltas[layer+1]\n",
    "\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for layer in range(len(nn_structure)-1,0,-1):\n",
    "            W[layer] += -alpha*(delta_W[layer]/m)\n",
    "            b[layer] += -alpha*(delta_b[layer]/m)\n",
    "\n",
    "        mse_cost = cost / m\n",
    "        mse_cost_overtime.append(mse_cost)\n",
    "        \n",
    "        # print the iteration number for every 1000 iter\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"At iteration\", epoch, \"loss is \", mse_cost_overtime[epoch])\n",
    "\n",
    "    return W, b, mse_cost_overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_y(W, b, X, n_layer_idx):\n",
    "    m, n = X.shape\n",
    "    y_pred = np.zeros((m,))\n",
    "\n",
    "    for i, data in enumerate(X):\n",
    "        a, _ = feed_forward(data, W, b)\n",
    "        y_pred[i] = np.argmax(a[n_layer_idx])\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the neural network\n",
    "\n",
    "Our code assumes the size of each layer in our network is held in a list.  The input layer will have 64 neurons (one for each pixel in our 8 by 8 pixelated digit).  Our hidden layer has 30 neurons (you can change this value).  The output layer has 10 neurons.\n",
    "\n",
    "Next we create the python list to hold the number of neurons for each level and then run the neural network code with our training data.\n",
    "\n",
    "This code will take some time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 3000 iterations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1db104e49c4e3cbc0d8b3c908d8542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0 loss is  2.811712792488314\n",
      "At iteration 500 loss is  0.9003469931616888\n",
      "At iteration 1000 loss is  0.7592722276884563\n",
      "At iteration 1500 loss is  0.6366614449841789\n",
      "At iteration 2000 loss is  0.5456306119333528\n",
      "At iteration 2500 loss is  0.47683253560956484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [64, 30, 10]\n",
    "    \n",
    "# train the NN with the nn_structure and 3000 iterations\n",
    "trained_weight, trained_bias, mse_cost_overtime = train_nn(nn_structure, X_train, y_train_vect, iter_num=3000, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the learning curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the avg_cost_func \n",
    "plt.plot(mse_cost_overtime)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Average J')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing accuracy\n",
    "Next we determine what percentage the neural network correctly predicted the handwritten digit correctly on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 88.456189%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_y(trained_weight, trained_bias, X_test, len(nn_structure))\n",
    "print('Prediction accuracy is {:5f}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
