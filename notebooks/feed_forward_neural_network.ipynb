{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "We will use the lower resolution MINST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "from collections.abc import Sequence\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits              # The MNIST data set\n",
    "from sklearn.preprocessing import StandardScaler      # It is important in neural networks to scale the date\n",
    "from sklearn.model_selection import train_test_split  # The standard train test split to prevent overfitting and choose hyperparameters\n",
    "\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the digits dataset: (1797, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load all the digits dataset from the sklearn library\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "print(\"The shape of the digits dataset:\", X.shape)\n",
    "\n",
    "def plt_digit(x, h, w):\n",
    "    plt.imshow(x.reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "def display_digits(X, height, width, num_images=10):\n",
    "    if num_images&1 or num_images==0:\n",
    "        raise ValueError(\"Only even values (>=2) for num_images allowed\")\n",
    "    fig = plt.figure()\n",
    "    for i in range(1, num_images+1):\n",
    "        fig.add_subplot(2, num_images//2, i)\n",
    "        plt_digit(X[i], height, width)\n",
    "    plt.gray()\n",
    "    plt.show()\n",
    "\n",
    "display_digits(X, 8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the dataset\n",
    "To speed the weight convergence, the training features must be scaled to have a mean of 0 and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.33501649, -0.04308102, ..., -1.14664746,\n",
       "        -0.5056698 , -0.19600752],\n",
       "       [ 0.        , -0.33501649, -1.09493684, ...,  0.54856067,\n",
       "        -0.5056698 , -0.19600752],\n",
       "       [ 0.        , -0.33501649, -1.09493684, ...,  1.56568555,\n",
       "         1.6951369 , -0.19600752],\n",
       "       ...,\n",
       "       [ 0.        , -0.33501649, -0.88456568, ..., -0.12952258,\n",
       "        -0.5056698 , -0.19600752],\n",
       "       [ 0.        , -0.33501649, -0.67419451, ...,  0.8876023 ,\n",
       "        -0.5056698 , -0.19600752],\n",
       "       [ 0.        , -0.33501649,  1.00877481, ...,  0.8876023 ,\n",
       "        -0.26113572, -0.19600752]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the StandardScaler to standardize our X_feat\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Looking the new features after scaling\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and test datasets\n",
    "\n",
    "We split the data into training and test data sets with 60-40 split. \n",
    "\n",
    "We will train the neural network with the training dataset, and evaluate our neural network with the test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.40, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_to_vect(y):\n",
    "    \"\"\"\n",
    "    One Hot Encoding for target\n",
    "    Converts into a 10 element array\n",
    "    \n",
    "    y can be an integer or a Sequence of integers\n",
    "    \"\"\"\n",
    "    if isinstance(y, int):\n",
    "        return np.array([1 if i==y else 0 for i in range(10)])\n",
    "    elif isinstance(y, Sequence) or isinstance(y, np.ndarray):\n",
    "        return np.array([[1 if i==target else 0 for i in range(10)] for target in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the training and test targets to vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert digits to vectors using the func above\n",
    "y_train_vect, y_test_vect = convert_y_to_vect(y_train), convert_y_to_vect(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5 4 4]\n",
      "[[0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    print(y_train[0:4])\n",
    "    print((y_train_vect[0:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Activation Functions\n",
    "\n",
    "**The activation function and its derivative**\n",
    "\n",
    "Sigmoid activation function $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "Sigmoid derivative function $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$\n",
    "\n",
    "RELU activation function $relu(z) = max(0, m*z)$ where m=1\n",
    "\n",
    "RELU derivative function $  \n",
    "relu'(z) = \n",
    "     \\begin{cases}\n",
    "       \\text{0} &\\quad\\text{if z} \\ge 0 \\\\\n",
    "       \\text{1} &\\quad\\text{if z} \\lt 0 \\\\\n",
    "     \\end{cases}\n",
    "$\n",
    "\n",
    "Tanh activation function $tanh(z) = \\frac{e^{2z}-1}{e^{2z}+1}$\n",
    "\n",
    "Tnah derivative function $tanh'(z) = 1 + tanh(z)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.e**(-z))\n",
    "\n",
    "\n",
    "def sigmoid_deriv(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "\n",
    "def tanh(z):\n",
    "    return (np.e**(2*z)-1)/(np.e**(2*z)+1)\n",
    "\n",
    "\n",
    "def tanh_deriv(z):\n",
    "    return 1 - tanh(z)**2\n",
    "\n",
    "\n",
    "def relu(z, m=0):\n",
    "    \"\"\"\n",
    "    default RELU = max(0, z) and m=0\n",
    "    \"\"\"\n",
    "    return np.where(x > 0, x, x * m)\n",
    "\n",
    "\n",
    "def relu_deriv(z, m=0):\n",
    "    z[z < 0] = m\n",
    "    z[z == 0] = 0\n",
    "    z[z > 0] = 1\n",
    "    return z\n",
    "\n",
    "leaky_relu = partial(relu, m=0.01)\n",
    "leaky_relu_deriv = partial(relu_deriv, m=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(y_pred, y_test):\n",
    "    return np.sqrt((np.sum(y_pred-y_test))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and initializing W and b\n",
    "\n",
    "W = weights of the neurons\n",
    "\n",
    "b = biases of the neurons\n",
    "\n",
    "The weights in W are different so that during back propagation, the nodes on a level will have different gradients and thus have different update values.\n",
    "\n",
    "The weights are randomly initialized from the uniform range \\[0.0, 1.0\\). The weights have to be small as the sigmoid flats out for large inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_and_bias(nn_structure: List[int]):\n",
    "    \"\"\"\n",
    "    nn_structure is a list that reprs the number of neurons in the NN layer\n",
    "    We use the Kaiming Initialization for the weights\n",
    "    \"\"\"\n",
    "    weights, bias = {}, {}\n",
    "    \n",
    "    # first layer is input layer so we do not save weights for it\n",
    "    for layer, n_neurons in enumerate(nn_structure[1:], start=1):\n",
    "        weights[layer] = np.random.random((n_neurons, nn_structure[layer-1])) / np.sqrt(n_neurons)\n",
    "        bias[layer] = np.random.random(n_neurons)\n",
    "        \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the weight and bias gradient dicts $\\triangledown W$ and $\\triangledown b$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_and_bias_gradients(nn_structure: List[int]):\n",
    "    delta_W, delta_b = {}, {}\n",
    "    \n",
    "    for layer, n_neurons in enumerate(nn_structure[1:], start=1):\n",
    "        delta_W[layer] = np.zeros((n_neurons, nn_structure[layer-1]))\n",
    "        delta_b[layer] = np.zeros(n_neurons)\n",
    "        \n",
    "    return  delta_W, delta_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward\n",
    "\n",
    "The feed_forward function returns the values of $a$ and $z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, W, b):\n",
    "    \"\"\"\n",
    "    x is a single data point / 1D array\n",
    "    \"\"\"\n",
    "    # a in layer 1 is the input x itself\n",
    "    a = {1:x} # holds values of 'a' for all layers, a = func_activation(z)\n",
    "    z = {}    # holds values of 'z' for all layers, z = Wx + b\n",
    "    X_feat = x\n",
    "\n",
    "    for layer, (weight,bias) in enumerate(zip(W, b), start=2): \n",
    "        z[layer] = W[layer-1].dot(X_feat) + b[layer-1]\n",
    "        a[layer] = sigmoid(z[layer])\n",
    "        X_feat = a[layer]\n",
    "        \n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing $\\delta$\n",
    "\n",
    "$\\delta^{(s_l)}$ is computed in \"calculate_out_layer_delta\"  \n",
    "\n",
    "$delta^{nl} = -(y_i - a_i^{nl}) * f'(z_i^{nl})$\n",
    "\n",
    "$\\delta^{(\\ell)}$ is computed for the hidden layers in \"calculate_hidden_delta\" \n",
    "    \n",
    "$delta^{l} = (transpose(W^{l}) * delta^{l+1}) * f'(z^{l})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    # delta^(nl) = -(y_i - a_i^(nl)) * f'(z_i^(nl))\n",
    "    return  -(y-a_out) * sigmoid_deriv(z_out)\n",
    "\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return (w_l.T@delta_plus_1) * sigmoid_deriv(z_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Back Propagation Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25):\n",
    "    \"\"\"\n",
    "    X must be np.ndarray\n",
    "    \"\"\"\n",
    "    W, b = init_weight_and_bias(nn_structure)\n",
    "    m, n = X.shape\n",
    "    mse_cost_overtime = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    for epoch in tqdm(range(iter_num)):\n",
    "        delta_W, delta_b = init_weight_and_bias_gradients(nn_structure)\n",
    "        cost = 0\n",
    "        \n",
    "        for data_idx, data in enumerate(X):\n",
    "            # feed forward pass saves a and z values to be used in gradient descent\n",
    "            a, z = feed_forward(data, W, b)\n",
    "            deltas = {}\n",
    "            # loop from n-1 to 1 backpropagating the errors\n",
    "            for layer in range(len(nn_structure),0,-1):\n",
    "                if layer == len(nn_structure): # layer is output layer\n",
    "                    deltas[layer] = calculate_out_layer_delta(y[data_idx,:], a[layer], z[layer])\n",
    "                    # squared error calculation\n",
    "                    cost += np.sqrt(np.sum((a[layer] - y[data_idx,:])**2))\n",
    "                else:\n",
    "                    if layer > 1: # layer is hidden layer\n",
    "                        deltas[layer] = calculate_hidden_delta(deltas[layer+1], W[layer], z[layer])\n",
    "                        \n",
    "                    # np.newaxis increases the number of dimensions\n",
    "                    # delta_W^(l) = delta_W^(l) + deltas^(l+1) * transpose(a^(l))\n",
    "                    delta_W[layer] +=  (deltas[layer+1][:, np.newaxis] @ a[layer][:,np.newaxis].T)\n",
    "                    # delta_b^(l) = delta_b^(l) + deltas^(l+1)\n",
    "                    delta_b[layer] +=  deltas[layer+1]\n",
    "\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for layer in range(len(nn_structure)-1,0,-1):\n",
    "            W[layer] += -alpha*(delta_W[layer]/m)\n",
    "            b[layer] += -alpha*(delta_b[layer]/m)\n",
    "\n",
    "        mse_cost = cost / m\n",
    "        mse_cost_overtime.append(mse_cost)\n",
    "        \n",
    "        # print the iteration number for every 1000 iter\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"At iteration\", epoch, \"loss is \", mse_cost_overtime[epoch])\n",
    "\n",
    "    return W, b, mse_cost_overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_y(W, b, X, n_layer_idx):\n",
    "    m, n = X.shape\n",
    "    y_pred = np.zeros((m,))\n",
    "\n",
    "    for i, data in enumerate(X):\n",
    "        a, _ = feed_forward(data, W, b)\n",
    "        y_pred[i] = np.argmax(a[n_layer_idx])\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the neural network\n",
    "\n",
    "Our code assumes the size of each layer in our network is held in a list.  The input layer will have 64 neurons (one for each pixel in our 8 by 8 pixelated digit).  Our hidden layer has 30 neurons (you can change this value).  The output layer has 10 neurons.\n",
    "\n",
    "Next we create the python list to hold the number of neurons for each level and then run the neural network code with our training data.\n",
    "\n",
    "This code will take some time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 3000 iterations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51b1df71cb5471eb20a7ff80b8bc4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0 loss is  2.8581704272857276\n",
      "At iteration 100 loss is  0.9539796619543467\n",
      "At iteration 200 loss is  0.9427645883084591\n",
      "At iteration 300 loss is  0.9248125150672019\n",
      "At iteration 400 loss is  0.8890826495482642\n",
      "At iteration 500 loss is  0.828854432299645\n",
      "At iteration 600 loss is  0.7579391394064134\n",
      "At iteration 700 loss is  0.6913043134596762\n",
      "At iteration 800 loss is  0.6322265008029798\n",
      "At iteration 900 loss is  0.5825980040770571\n",
      "At iteration 1000 loss is  0.541418400058578\n",
      "At iteration 1100 loss is  0.506707445996259\n",
      "At iteration 1200 loss is  0.4766670550336959\n",
      "At iteration 1300 loss is  0.44964784455872614\n",
      "At iteration 1400 loss is  0.42459101134558247\n",
      "At iteration 1500 loss is  0.40125371111350494\n",
      "At iteration 1600 loss is  0.37989030946599556\n",
      "At iteration 1700 loss is  0.36069818791226493\n",
      "At iteration 1800 loss is  0.34364031269054246\n",
      "At iteration 1900 loss is  0.3285268702799243\n",
      "At iteration 2000 loss is  0.3150795449685798\n",
      "At iteration 2100 loss is  0.3030796393132645\n",
      "At iteration 2200 loss is  0.29231327728661904\n",
      "At iteration 2300 loss is  0.2825885637447202\n",
      "At iteration 2400 loss is  0.2737476249347336\n",
      "At iteration 2500 loss is  0.26566529108377446\n",
      "At iteration 2600 loss is  0.2582410910966615\n",
      "At iteration 2700 loss is  0.2513885972834286\n",
      "At iteration 2800 loss is  0.24503497079784536\n",
      "At iteration 2900 loss is  0.23911989048189025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [64, 30, 10]\n",
    "    \n",
    "# train the NN with the nn_structure and 3000 iterations\n",
    "trained_weight, trained_bias, mse_cost_overtime = train_nn(nn_structure, X_train, y_train_vect, iter_num=3000, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the learning curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the avg_cost_func \n",
    "plt.plot(mse_cost_overtime)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Average J')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing accuracy\n",
    "Next we determine what percentage the neural network correctly predicted the handwritten digit correctly on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 95.688456%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_y(trained_weight, trained_bias, X_test, len(nn_structure))\n",
    "print('Prediction accuracy is {:5f}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network OOP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     2,
     12,
     21,
     33,
     53,
     59,
     65,
     68
    ]
   },
   "outputs": [],
   "source": [
    "class FCNeuralNetwork:\n",
    "\n",
    "    def __init__(self, nn_structure: List[int], act_func=sigmoid, act_func_deriv=sigmoid_deriv) -> None:\n",
    "        \"\"\"\n",
    "        nn_structure is a list that reprs the NNs FC layers [64,50,30,10] where 64 is the input layer\n",
    "        \"\"\"\n",
    "        self.nn_structure = nn_structure\n",
    "        self.weight, self.bias = {}, {}\n",
    "        self.a_func = act_func\n",
    "        self.a_func_deriv = act_func_deriv\n",
    "        self.mse_overtime = []\n",
    "\n",
    "    def _init_weight_and_bias(self) -> None:\n",
    "        \"\"\"\n",
    "        We use Kaiming initialization and the first layer is skipped as it is the input layer\n",
    "        \"\"\"\n",
    "        for layer, n_neurons in enumerate(self.nn_structure[1:], start=1):\n",
    "            self.weight[layer] = np.random.random(\n",
    "                (n_neurons, self.nn_structure[layer-1])) / np.sqrt(n_neurons)\n",
    "            self.bias[layer] = np.random.random(n_neurons)\n",
    "\n",
    "    def _gen_weight_and_bias_deltas(self) -> Tuple[dict, dict]:\n",
    "        \"\"\"\n",
    "        Skip first layer as it is the input layer\n",
    "        \"\"\"\n",
    "        W_delta, b_delta = {}, {}\n",
    "        for layer, n_neurons in enumerate(self.nn_structure[1:], start=1):\n",
    "            W_delta[layer] = np.zeros(\n",
    "                (n_neurons, self.nn_structure[layer-1]))\n",
    "            b_delta[layer] = np.zeros(n_neurons)\n",
    "\n",
    "        return W_delta, b_delta\n",
    "\n",
    "    def _feedforward(self, data) -> Tuple[dict, dict]:\n",
    "        \"\"\"\n",
    "        z = Wx + b\n",
    "        a = f(z) where f = activation function\n",
    "        The first layer in a stores the input values, data itself\n",
    "\n",
    "        self.weight and self.bias both start from layer 1\n",
    "        \"\"\"\n",
    "        if self.weight == {} or self.bias == {}:\n",
    "            raise ValueError(\n",
    "                \"Weights and biases of the network have not been initialized\")\n",
    "\n",
    "        a, z = {1: data}, {}\n",
    "        X_feat = data\n",
    "        for layer, (weight, bias) in enumerate(zip(self.weight, self.bias), start=2):\n",
    "            z[layer] = self.weight[layer-1].dot(X_feat) + self.bias[layer-1]\n",
    "            a[layer] = self.a_func(z[layer])\n",
    "            X_feat = a[layer]\n",
    "        return a, z\n",
    "\n",
    "    def _cal_out_layer_delta(self, y_vect, a_out, z_out):\n",
    "        \"\"\"\n",
    "        delta^(nth layer) = -(y_i - a_i^(nth layer)) * f'(z_i^(nth layer))\n",
    "        \"\"\"\n",
    "        return -(y_vect-a_out) * self.a_func_deriv(z_out)\n",
    "\n",
    "    def _cal_hidden_layer_delta(self, delta_layer_plus_one, W_layer, z_layer):\n",
    "        \"\"\"\n",
    "        delta^(layer) = (transpose(W^(layer)) * delta^(layer+1)) * f'(z^(layer))\n",
    "        \"\"\"\n",
    "        return (W_layer.T@delta_layer_plus_one)*self.a_func_deriv(z_layer)\n",
    "\n",
    "    def accuracy(self, y_pred, y_test):\n",
    "        return accuracy_score(y_test, y_pred)*100\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        m, n = X_test.shape\n",
    "        y_pred = np.zeros(m)\n",
    "        for i, data in enumerate(X_test):\n",
    "            predicted, _ = self._feedforward(data)\n",
    "            final_layer_output = predicted[len(self.nn_structure)]\n",
    "            # As output is number in interval [0,9]\n",
    "            y_pred[i] = np.argmax(final_layer_output)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, epochs=3000, alpha=0.25) -> None:\n",
    "        self._init_weight_and_bias()\n",
    "        m, n = X.shape\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            cost = 0\n",
    "            delta_W, delta_b = self._gen_weight_and_bias_deltas()\n",
    "            \n",
    "            for data_idx, data in enumerate(X):\n",
    "                a, z = self._feedforward(data)  # feedfoward data\n",
    "                deltas = {}\n",
    "\n",
    "                # backpropagation upto to layer 1\n",
    "                for layer in range(len(self.nn_structure), 0, -1):\n",
    "                    if layer == len(self.nn_structure):  # Output layer\n",
    "                        y_vect = y[data_idx, :]  # actual y_train value\n",
    "                        deltas[layer] = self._cal_out_layer_delta(\n",
    "                            y_vect, a[layer], z[layer])\n",
    "                        cost += squared_error(a[layer], y_vect)\n",
    "                    else:\n",
    "                        if layer > 1:  # if layer is hidden layer and not input layer\n",
    "                            deltas[layer] = self._cal_hidden_layer_delta(deltas[layer+1],self.weight[layer],z[layer])\n",
    "                        \n",
    "                        # np.newaxis increases the number of dimensions\n",
    "                        # delta_W^(layer) = delta_W^(layer) + deltas^(layer+1) * transpose(a^(layer))\n",
    "                        delta_W[layer] += (deltas[layer+1][:, np.newaxis]\n",
    "                                           @ a[layer][:, np.newaxis].T)\n",
    "                        # delta_b^(layer) = delta_b^(layer) + deltas^(layer+1)\n",
    "                        delta_b[layer] += deltas[layer+1]\n",
    "\n",
    "            # gradient descent update for layer 1 to layer n-1\n",
    "            for layer in range(1, len(self.nn_structure)-1):\n",
    "                self.weight[layer] += -alpha * (delta_W[layer]/m)\n",
    "                self.bias[layer] += -alpha * (delta_b[layer]/m)\n",
    "\n",
    "            # calculate mse cost\n",
    "            mse_cost = cost / m\n",
    "            self.mse_overtime.append(mse_cost)\n",
    "\n",
    "            # Print current cost at epoch intervals of 100\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"At epoch {epoch}, mse-cost is {mse_cost}\")\n",
    "        print(f\"At epoch {epochs}, mse-cost is {mse_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_digits(return_X_y=True)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.40, random_state=1)\n",
    "\n",
    "y_train_vect = convert_y_to_vect(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_network_sigmoid = FCNeuralNetwork([64, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092cbca10dc7492f90a43f65031fb28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0, mse-cost is 8.637873584275045\n",
      "At epoch 100, mse-cost is 8.382494844797197\n",
      "At epoch 200, mse-cost is 7.999367054582114\n",
      "At epoch 300, mse-cost is 7.705468468734416\n",
      "At epoch 400, mse-cost is 7.497983356982718\n",
      "At epoch 500, mse-cost is 7.328218400629312\n",
      "At epoch 600, mse-cost is 7.170066540817601\n",
      "At epoch 700, mse-cost is 7.017607732646176\n",
      "At epoch 800, mse-cost is 6.871688145726446\n",
      "At epoch 900, mse-cost is 6.73595942060322\n",
      "\n",
      "At epoch 1000, mse-cost is 6.618444854983972\n"
     ]
    }
   ],
   "source": [
    "nn_network_sigmoid.fit(X_train, y_train_vect, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.265646731571627"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = nn_network_sigmoid.predict(X_test)\n",
    "nn_network_sigmoid.accuracy(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8., 9., 8., 5., 5., 5., 5., 5., 8., 5., 9., 9., 8., 5., 5., 5., 5.,\n",
       "       9., 5., 5., 5., 8., 8., 5., 5., 5., 5., 5., 9., 5., 5., 9., 9., 8.,\n",
       "       9., 8., 5., 5., 8., 5., 5., 5., 5., 9., 8., 5., 9., 5., 8., 8., 8.,\n",
       "       9., 5., 5., 8., 5., 5., 5., 8., 9., 9., 5., 9., 8., 8., 8., 5., 9.,\n",
       "       9., 8., 8., 8., 5., 5., 5., 8., 9., 5., 5., 9., 8., 5., 9., 5., 9.,\n",
       "       9., 5., 9., 9., 9., 9., 9., 9., 5., 5., 8., 8., 5., 8., 9., 9., 5.,\n",
       "       9., 8., 9., 5., 9., 5., 9., 9., 9., 9., 9., 5., 8., 5., 9., 9., 5.,\n",
       "       5., 5., 5., 5., 5., 5., 5., 5., 5., 8., 5., 5., 5., 8., 9., 8., 8.,\n",
       "       8., 9., 5., 5., 9., 5., 8., 5., 8., 5., 5., 5., 5., 9., 5., 9., 8.,\n",
       "       5., 9., 9., 9., 8., 5., 8., 8., 9., 5., 8., 5., 5., 8., 8., 8., 9.,\n",
       "       5., 5., 5., 8., 5., 9., 5., 5., 5., 5., 5., 5., 9., 5., 9., 5., 9.,\n",
       "       5., 5., 9., 9., 5., 8., 5., 9., 8., 5., 5., 5., 5., 5., 5., 9., 8.,\n",
       "       8., 9., 5., 5., 5., 5., 9., 5., 8., 5., 5., 5., 5., 8., 9., 8., 9.,\n",
       "       8., 8., 8., 8., 9., 5., 8., 9., 8., 5., 9., 9., 9., 8., 9., 5., 8.,\n",
       "       5., 9., 9., 9., 8., 9., 9., 5., 5., 8., 9., 5., 5., 5., 9., 8., 5.,\n",
       "       5., 5., 8., 9., 8., 5., 9., 9., 8., 8., 5., 8., 5., 5., 5., 8., 5.,\n",
       "       9., 8., 8., 5., 5., 5., 8., 5., 5., 5., 8., 5., 8., 5., 9., 8., 9.,\n",
       "       5., 8., 8., 5., 9., 5., 9., 5., 8., 5., 5., 5., 8., 9., 9., 5., 5.,\n",
       "       5., 9., 5., 5., 9., 8., 5., 9., 9., 8., 8., 8., 5., 5., 5., 5., 5.,\n",
       "       5., 8., 9., 8., 8., 8., 8., 8., 5., 8., 8., 5., 5., 8., 9., 9., 8.,\n",
       "       9., 8., 5., 5., 8., 5., 8., 5., 9., 5., 5., 8., 5., 8., 9., 5., 8.,\n",
       "       5., 9., 8., 5., 5., 8., 8., 9., 5., 5., 8., 5., 5., 8., 8., 9., 9.,\n",
       "       9., 8., 9., 9., 8., 5., 5., 5., 5., 9., 8., 9., 9., 9., 8., 5., 5.,\n",
       "       5., 8., 9., 9., 9., 8., 8., 5., 5., 8., 9., 5., 5., 5., 8., 8., 5.,\n",
       "       5., 8., 5., 8., 9., 5., 5., 8., 5., 5., 8., 5., 9., 8., 5., 5., 8.,\n",
       "       5., 9., 5., 5., 5., 5., 9., 8., 9., 9., 5., 5., 5., 5., 5., 5., 9.,\n",
       "       8., 9., 8., 8., 9., 8., 8., 8., 8., 8., 5., 5., 8., 5., 8., 5., 5.,\n",
       "       5., 9., 8., 9., 9., 9., 8., 9., 8., 9., 8., 5., 5., 5., 5., 8., 8.,\n",
       "       9., 9., 9., 5., 5., 8., 5., 8., 8., 5., 8., 9., 5., 9., 5., 9., 9.,\n",
       "       5., 5., 9., 5., 8., 8., 9., 5., 5., 9., 5., 9., 5., 8., 5., 5., 9.,\n",
       "       9., 5., 8., 5., 5., 5., 5., 8., 9., 9., 8., 9., 8., 8., 5., 8., 8.,\n",
       "       8., 9., 8., 5., 5., 5., 9., 5., 9., 8., 9., 5., 5., 8., 8., 8., 8.,\n",
       "       5., 5., 8., 8., 5., 5., 5., 9., 9., 9., 8., 5., 9., 5., 8., 9., 8.,\n",
       "       5., 8., 9., 9., 8., 9., 5., 5., 5., 9., 5., 9., 5., 5., 5., 5., 8.,\n",
       "       5., 5., 8., 9., 8., 8., 5., 8., 9., 5., 8., 8., 5., 8., 5., 5., 5.,\n",
       "       5., 8., 8., 9., 5., 5., 5., 5., 8., 5., 8., 5., 5., 8., 9., 8., 8.,\n",
       "       8., 8., 9., 9., 8., 9., 9., 8., 8., 9., 8., 9., 5., 9., 5., 9., 8.,\n",
       "       9., 5., 8., 8., 8., 9., 8., 9., 5., 8., 5., 5., 8., 5., 5., 8., 8.,\n",
       "       8., 8., 8., 9., 5., 5., 5., 9., 9., 9., 9., 9., 8., 5., 5., 5., 9.,\n",
       "       8., 8., 8., 8., 8., 9., 5., 8., 5., 9., 5., 5., 5., 5., 8., 8., 5.,\n",
       "       5., 8., 5., 5., 8., 5., 5., 5., 5., 5., 9., 9., 9., 9., 5., 5., 8.,\n",
       "       9., 9., 5., 5., 5., 8., 9., 5., 9., 8., 5., 8., 9., 8., 5., 5., 5.,\n",
       "       5., 5., 9., 5., 5.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
